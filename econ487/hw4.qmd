---
title: "HW4"
output: html_document
date: "2023-10-25"
author: "Lesley Xu"
---

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyverse)
library(broom)
library(janitor)
library(knitr)
library(glmnet)
```
1) Take a model that includes brand, feat, log(price), their interactions, lagged price, and demographics, and fit a LASSO using glmnet which is a workhorse R package for LASSO, Ridge and Elastic Nets.   
a.	First remember to install the glmnet package and library to your R session.    
b.	Remember to estimate a LASSO you must pass glmnet a matrix of data for candidate features and a vector as candidate outcomes: As an alternative to defining products in a dataframe, turning that into a matrix then passing that matrix to glmnet, you can cut out the middle man with this. In addition to the variables in the original dataframe, try to create tons of new features that you think could plausibly be predictive of quantity sold. This could include lagged prices, interactions of several features, etc.

```{r, message=FALSE, warning=FALSE}
setwd("/Users/lesleyxu/Desktop/Past courses/23AU/ECON 487")
oj <- read_csv('oj.csv', show_col_types = FALSE) %>%
  clean_names()

demo_cols <- oj %>% 
  select(age60:hval150) %>% 
  colnames()

oj_cv <- oj %>% 
  dplyr::mutate(log_price = log(price)) %>% 
  dplyr::arrange(week) %>% # sort the data by week
  dplyr::group_by(store, brand) %>% # only lag within a store and brand
  dplyr::mutate(lag_price = ifelse(lag(week) + 1 == week, lag(log_price), NA)) %>% # calculate lagged prices only if subsequent weeks
  dplyr::ungroup() %>% 
  dplyr::filter(!is.na(lag_price)) %>%  # remove observations without a lagged price
  dplyr::mutate(dataset_id = sample(
           rep(c(1:5), ceiling(n()/5)), 
           size = n(), 
           replace = FALSE
         ))

reg_int <- str_c('logmove ~ log_price*brand*feat + lag_price + ' , str_c(demo_cols, collapse = ' + '))
set.seed(720)
x <- model.matrix(formula(reg_int), oj_cv)
y <- oj_cv$logmove

lasso_v1 <- glmnet(x, y, alpha=1)
#Results
plot(lasso_v1)
```


```{r, message=FALSE, warning=FALSE}
# Now ready for cross validation version of the object
cvfit <- cv.glmnet(x, y, alpha=1)
#Results
plot(cvfit)
```

c.	Investigate the coefficients of the cross validated LASSO model.
Which are the parameters the cross validated LASSO model kicks out of the model? What is the ratio of number of features to number of observations?  How might that relate to overfitting from “sampling error”?
```{r, message=FALSE, warning=FALSE}
summary(head((coef(lasso_v1, s=lasso_v1$lambda.min))))
```

```{r, message=FALSE, warning=FALSE}
log(cvfit$lambda.min)
coef(cvfit, s = "lambda.min")
```

d.	Can you look that the glmnet objects and figure out what the out of sample (e.g., test set) average MSE was with the cross validated LASSO model relative to the model in 1.c?
```{r, message=FALSE, warning=FALSE}
cvfit
oj_reg_demo <- oj %>% 
  dplyr::mutate(id_val = row_number(),
         log_price = log(price))

df_train <- oj_reg_demo %>% 
  slice_sample(prop = .8)

df_test <- oj_reg_demo %>% 
  anti_join(df_train,
            by = 'id_val')

reg_str <- str_c('logmove ~ log_price*feat*brand + ', str_c(demo_cols, collapse = ' + '))

reg_form <- formula(reg_str)
demo_reg_train <- lm(reg_form, data = df_train)

reg_train <- lm(logmove ~ log_price*feat*brand, data = df_train)

mse <- function(model, test_set){
  return(round(mean((test_set$logmove - predict(model, newdata=test_set))^2), 2))
}

mse(demo_reg_train, df_test)
```
- The average MSE with the cross validated LASSO model is 0.4419, and the MSE for test set with demographic variables included from last problem is 0.45. Therefore, the LASSO model did a better job since it has a lower MSE. 

e.	What is the advantage of using LASSO for choosing model complexity as opposed to using your intuition as an economist?  

i.	In what part of this process did you use your intuition as an economist? (HINT: what’s in the X matrix?)

- By using LASSO, I am able to reduce as much coefficient to zero as possible, and the left non-zero coefficient will be the more significant variables in my model. LASSO shrinks the coefficients of less important variables to zero while keep the important variables. And because the model selected according to LASSO will be less complex, it will help avoid overfitting at the same time. 

2) Now estimate the model with only the variable selected with the LASSO procedure but with OLS to avoid attenuation bias in the coefficients (similar to this paper).  
a.	Let’s return to the orange juice assignment and get very precise about how to interpret coefficients.  What is the predicted elasticity in the following cases? 

  i.	For Dominicks when the lagged price is $1 (NOTE: did you interact lagged price with current period price?)  If not, does lagged price impact the elasticity this period or log move this period.
```{r, message=FALSE, warning=FALSE}
coefficients <- coef(cvfit, s = cvfit$lambda.1se)
# extract the variables with nonzero coefficients
rownames(coef(cvfit, s = 'lambda.1se'))[coef(cvfit, s = 'lambda.1se')[,1]!= 0] %>%
  kable()
```

```{r, message=FALSE, warning=FALSE}
oj_cv_price <- oj %>% 
  dplyr::mutate(log_price = log(price)) %>% 
  dplyr::arrange(week) %>% # sort the data by week
  dplyr::group_by(store, brand) %>% # only lag within a store and brand
  dplyr::mutate(lag_price = ifelse(lag(week) + 1 == week, lag(price), NA)) %>% # calculate lagged prices only if subsequent weeks
  dplyr::ungroup() %>% 
  dplyr::filter(!is.na(lag_price)) %>%  # remove observations without a lagged price
  dplyr::mutate(dataset_id = sample(
           rep(c(1:5), ceiling(n()/5)), 
           size = n(), 
           replace = FALSE
         ))

mod2 <- formula('logmove ~ log_price + + log(lag_price) + log_price*log(lag_price) + brand + brand*log_price + brand*feat + brand*feat*log_price + income + educ + hhlarge + workwom + age60 + feat + age60*income + age60*log_price + educ*log_price + hhlarge*log_price + workwom*log_price + hhlarge*workwom + educ*workwom + income*workwom + income*hhlarge')

dom_lag_1 <- oj_cv_price %>%
  filter(brand == 'dominicks') %>%
  filter(lag_price == 1)
```
  
  ii.	For Tropicana
  
```{r, message=FALSE, warning=FALSE}
trop_pred <- oj_cv_price %>%
  filter(brand == 'tropicana')
```
  
  iii.	For Tropicana when its featured
```{r, message=FALSE, warning=FALSE}
trop_feat <- oj_cv_price %>%
  filter(brand == 'tropicana') %>%
  filter(feat == 1)
```
  
  iv.	What is the 95% confidence intervals for Tropicana

```{r, message=FALSE, warning=FALSE}
reg2 <- lm(mod2, oj_cv_price)
ci_trop <- confint(reg2, "brandtropicana", level=0.95, oj_cv_price)
ci_trop %>%
  kable()
```

b.	Which product has the most elastic demand?  

i.	Should that product have the highest markup over costs or lowest markup over costs?  Why?
```{r, message=FALSE, warning=FALSE}

```


3) Go back to using logmove and log(price).  
a.	Estimate a 3x3 matrix own price and cross price elasticities for Dominicks, Minute Maid, and Tropicana using only the current week’s prices.  Be sure to estimate separate models for sales of Dominicks, MM and Tropicana (e.g., you’ll run three separate regressions with the same RHS variables but different LHS variables).  It doesn’t need to be overly complicated, but make sure there is an interpretable elasticity estimate.  NOTE: This will require three different regressions & add in socio demographic controls for each store. 
```{r, message=FALSE, warning=FALSE}
wide_data <- oj_reg_demo %>% 
  select(store, week, brand, log_price) %>% 
  pivot_wider(
    id_cols = c(store,week), 
    names_from = brand, 
    values_from=log_price
  )

cross_price_data <- oj_reg_demo %>% 
  select(store, week, logmove, brand) %>% 
  left_join(wide_data,
            by = c('store', 'week'))

# split the dataset according to brand
oj_dominicks <- oj_reg_demo %>%
  dplyr::filter(brand == 'dominicks')

oj_tropicana <- oj_reg_demo %>%
  dplyr::filter(brand == 'tropicana')

oj_minutemaid <- oj_reg_demo %>%
  dplyr::filter(brand == 'minute.maid')
```

```{r, message=FALSE, warning=FALSE}
# rejoin the logmove and log price 
oj_dominicks$logprice_tropicana <- oj_tropicana$log_price
oj_dominicks$logmove_tropicana <- oj_tropicana$logmove
oj_dominicks$logprice_minutemaid <- oj_minutemaid$log_price
oj_dominicks$logmove_minutemaid <- oj_minutemaid$logmove

demo_cols_cross <- oj %>% 
  select(age60:cpwvol5) %>% 
  colnames()

reg_domi <- str_c('logmove ~ log_price + logprice_minutemaid + logprice_tropicana + ' , str_c(demo_cols_cross, collapse = ' + '))
model.dominicks <- lm(formula(reg_domi),oj_dominicks)

reg_trop <- str_c('logmove_tropicana ~ log_price + logprice_minutemaid + logprice_tropicana + ' , str_c(demo_cols_cross, collapse = ' + '))
model.tropicana <- lm(formula(reg_trop),oj_dominicks)

reg_minu <- str_c('logmove_minutemaid ~ log_price + logprice_minutemaid + logprice_tropicana + ' , str_c(demo_cols_cross, collapse = ' + '))
model.minutemaid <- lm(formula(reg_minu),oj_dominicks)

epsilon_d <- c(summary(model.dominicks)$coefficients[2:4])
epsilon_t <- c(summary(model.tropicana)$coefficients[2:4])
epsilon_m <- c(summary(model.minutemaid)$coefficients[2:4])

matrix_cross1 <- rbind(epsilon_d, epsilon_t, epsilon_m)

colnames(matrix_cross1) <- c('dominicks', 'minutemaid', 'tropicana')

matrix_cross1 %>%
  kable()
```

b.	Do the same but add in interactions for whether or not each brand is featured.
```{r, message=FALSE, warning=FALSE}
reg_domi_feat <- str_c('logmove ~ log_price + logprice_minutemaid + logprice_tropicana + feat + ' , str_c(demo_cols_cross, collapse = ' + '))
model.dominicks.feat <- lm(formula(reg_domi_feat),oj_dominicks)

reg_trop_feat <- str_c('logmove_tropicana ~ log_price + logprice_minutemaid + logprice_tropicana + feat +  ' , str_c(demo_cols_cross, collapse = ' + '))
model.tropicana.feat <- lm(formula(reg_trop_feat),oj_dominicks)

reg_minu_feat <- str_c('logmove_minutemaid ~ log_price + logprice_minutemaid + logprice_tropicana + feat + ' , str_c(demo_cols_cross, collapse = ' + '))
model.minutemaid.feat <- lm(formula(reg_minu_feat),oj_dominicks)

epsilon_d_feat <- c(summary(model.dominicks.feat)$coefficients[2:4])
epsilon_t_feat <- c(summary(model.tropicana.feat)$coefficients[2:4])
epsilon_m_feat <- c(summary(model.minutemaid.feat)$coefficients[2:4])

matrix_cross2 <- rbind(epsilon_d_feat, epsilon_t_feat, epsilon_m_feat)

colnames(matrix_cross2) <- c('dominicks', 'minutemaid', 'tropicana')

matrix_cross2 %>%
  kable()
```

i.	How do the estimates change?

- Comparing to the previous model without feature variable, the diagonal coefficients are decreased in magnitude (smaller absolute value), meaning when take feature into account, indicating a less responsiveness to change in price. 

ii.	What product’s sales suffer the most when Minute Maid is both featured and lowers its price?

- 

c.	Which two products are the most competitive with each other?  

- Dominicks and minute.maid are the most competitive with each other because the cross price elasticity are positive for those two brands, meaning that the increase in price of dominicks will lead to increase in price of minute.maid. They are substitutes, therefore the most competitive with each other. 

i.	How did you infer that looking at the cross price elasticity? 
ii.	What do you expect that to mean about the correlation of the prices of those two products?  Would they be more correlated or less correlated than the price of other pairs of products?

4) Create a sales weighted price for orange juice by store.  
a.	You’ll first need to create actual sales (call it “Q”) instead of log sales for the weighting and put it into your dataframe.  
```{r, message=FALSE, warning=FALSE}
oj_with_Q <- oj %>%
  mutate(Q = exp(logmove))
```

b.	You can use the weighted.mean() function for each store-week combination in the dplyr library.
```{r, message=FALSE, warning=FALSE}
library(plyr)
oj_weighted <- ddply(oj_with_Q, c('store','week'), function(x) c(weighted_mean = weighted.mean(x$price,x$Q)))
oj_with_Q <- left_join(oj_with_Q, oj_weighted, by=c('store', 'week'))
```

5) Now use oj$weighted_price as the LHS variable in a regression tree to predict differences in sales weight prices with store demographics as RHS variables.  Note that you’ll only need to do for a single brand since weighted price and sociodemographic variables are identical across brands within a store. 
a.	There are a couple libraries you’ll need which you’ll see in the lecture notes (rpart, maptree, etc.)
b.	There are two main pieces of code:
`dataToPass<-oj[,c("weighted_mean","AGE60","EDUC","ETHNIC","INCOME","HHLARGE","WORKWOM","HVAL150","SSTRDIST","SSTRVOL","CPDIST5","CPWVOL5")]`
The above creates a dataframe from the existing one (with weighted mean merged back in) which will then be passed into rpart (tree partitioning algorithm).  
`fit<-rpart(as.formula(weighted_mean ~ .),data=dataToPass,method="anova",cp=0.007)`
This is the code which will fit the tree.
c.	Play around with a couple different complexity parameters to get a feel for the data
`draw.tree(fit)` #This draws the tree
d.	Choose three different leaves to group stores into based upon what explains sales weighted price. 
i.	Assign each store to one of these leaves (we used this code previously).

```{r, message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)
library(maptree)
dataToPass <- oj_with_Q[,c("weighted_mean","age60","educ","ethnic","income","hhlarge","workwom","hval150","sstrdist","sstrvol","cpdist5","cpwvol5")]
treefit <- rpart(as.formula(weighted_mean ~ .),data=dataToPass,method="anova",cp=0.007)
draw.tree(treefit)
# alternative plot
rpart.plot(treefit)
```
 
```{r, message=FALSE, warning=FALSE}
dataToPass$leaf = treefit$where
# extract the variables regarding hhlarge and educ from last problem set
treefit2 <- rpart(as.formula(weighted_mean ~ educ + hhlarge),data=dataToPass,method="anova",cp=0.007)
draw.tree(treefit2)
rpart.plot(treefit2)
```


6) Estimate the own price elasticities for each one of the store buckets/leaves using the preferred specification:

a.	Now estimate cross price elasticities jointly with own price elasticities.  This means you must create a dataframe which has the prices of all types of OJ at the store.  (e.g., you should be able to use the Trop_Cross code you’ve used previously.
b.	You’ll also have to run 3 separate regressions for each leaf for a total of nine regressions.  
```{r, message=FALSE, warning=FALSE}
reg_int <- glm(logmove~log_price*feat + logprice_tropicana*feat + logprice_minutemaid*feat, data=oj_dominicks)
# treefit3 <- rpart(as.formula(logmove~log_price*feat + logprice_tropicana*feat + logprice_minutemaid*feat),data=oj_dominicks,method="anova",cp=0.007)
# draw.tree(treefit3)
# rpart.plot(treefit3)
```

In this example, we are investigating the own and cross price elasticities for Dominick’s brand (D) within leaf L.

i.	Save the coefficients for each leaf in a 3x3 matrix.  The diagonals will be own price elasticities and the off diagonals will be cross price elasticities.

ii.	There will be a unique 3x3 matrix for each leaf.

iii.	The 3x3 matrices WON’T be upper triangular because we’re estimating three unique regressions for each leaf.  
c.	Comment on any differences between own and cross price elasticities by leaf.  
```{r, message=FALSE, warning=FALSE}
# split the dataset according to the leave
oj_leaves <- split(dataToPass, dataToPass$leaf)
oj_leaf_2 <- data.frame(oj_leaves[[1]])
oj_leaf_4 <- data.frame(oj_leaves[[2]])
oj_leaf_5 <- data.frame(oj_leaves[[3]])

oj_leaf_2 <- left_join(oj_leaf_2, oj_with_Q, by=c("age60", "educ", "ethnic", "income", "hhlarge", "workwom", "hval150", "sstrdist", "sstrvol", "cpdist5", "cpwvol5", "weighted_mean"))
reg_int_2 <- glm(logmove ~ log(price) * brand * feat, data = oj_leaf_2)

oj_leaf_4 <- left_join(oj_leaf_4, oj_with_Q, by=c("age60", "educ", "ethnic", "income", "hhlarge", "workwom", "hval150", "sstrdist", "sstrvol", "cpdist5", "cpwvol5", "weighted_mean"))
reg_int_4 <- glm(logmove ~ log(price) * brand * feat, data = oj_leaf_4)

oj_leaf_5 <- left_join(oj_leaf_5, oj_with_Q, by=c("age60", "educ", "ethnic", "income", "hhlarge", "workwom", "hval150", "sstrdist", "sstrvol", "cpdist5", "cpwvol5", "weighted_mean"))
reg_int_5 <- glm(logmove ~ log(price) * brand * feat, data = oj_leaf_5)

epsilon_2 <- c(summary(reg_int_2)$coefficients[2:4])
epsilon_4 <- c(summary(reg_int_4)$coefficients[2:4])
epsilon_5 <- c(summary(reg_int_5)$coefficients[2:4])

epsilon_byleaf <- rbind(epsilon_2, epsilon_4, epsilon_5)

epsilon_byleaf %>%
  kable()
```


```{r, message=FALSE, warning=FALSE}
oj_with_leaves <- left_join(dataToPass, oj, by=c("age60", "educ", "ethnic", "income", "hhlarge", "workwom", "hval150", "sstrdist", "sstrvol", "cpdist5", "cpwvol5"))
```

7) Now let’s use the elasticities to think about pricing differentials.  


a.In the leaf with the highest own-price elasticities, what should the markups be relative to the other leafs? 

b.How do cross-price elasticities vary with the highest versus lowest own price elasticity leafs?  

i. What does this imply about differences in markups within high versus low elasticity stores across brands?
$$\text{markup} = \frac{\text{margin}}{\text{cost}}$$

ii.	Can you say anything about what this means for the timing of sales?  Should they occur at the same or different times across stores?

- It depends on the price sensitivity of target consumers. If they occur at the same time, the consumers might not have the incentive to purchase product from particular brand. 
